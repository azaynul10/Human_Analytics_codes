{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from imbalanced-learn) (1.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (0.10.2.post1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa  \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATADIR = r'C:\\Users\\Abedi\\OneDrive - Student Ambassadors\\archive (7)\\combined_datasets'\n",
    "CATEGORIES = ['fall', 'not fall']  \n",
    "\n",
    "def create_training_data():\n",
    "    training_data = []\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        for audio_file in os.listdir(path):\n",
    "            audio_path = os.path.join(path, audio_file)\n",
    "            \n",
    "            audio_array, sr = librosa.load(audio_path, sr=22050, duration=30)  # \n",
    "            \n",
    "            \n",
    "            time_features = extract_time_domain_features(audio_array)\n",
    "            freq_features = extract_frequency_domain_features(audio_array)\n",
    "            impact_features = extract_impact_features(audio_array, sr)\n",
    "            \n",
    "            combined_features = time_features + freq_features + impact_features\n",
    "            training_data.append([combined_features, class_num])\n",
    "    \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_impact_features(audio_array, sr):\n",
    "    onset_env = librosa.onset.onset_strength(y=audio_array, sr=sr)\n",
    "    return [np.max(onset_env), np.mean(onset_env)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'c:\\Users\\Abedi\\OneDrive - Student Ambassadors\\archive (7)\\combined_datasets' \n",
    "  \n",
    "CATEGORIES = ['fall', 'not fall']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def extract_time_domain_features(audio_array):\n",
    "    features = [\n",
    "        np.mean(audio_array),          \n",
    "        np.median(audio_array),         \n",
    "        np.var(audio_array),            \n",
    "        np.std(audio_array),            \n",
    "        skew(audio_array),              \n",
    "        kurtosis(audio_array),          \n",
    "        librosa.feature.zero_crossing_rate(audio_array).mean(),  \n",
    "        len(librosa.onset.onset_detect(y=audio_array))  \n",
    "    ]\n",
    "    print(f\"Time features extracted: {len(features)}\")  \n",
    "    return features\n",
    "\n",
    "def extract_frequency_domain_features(audio_array, sr):\n",
    "    features = [\n",
    "        np.mean(np.abs(librosa.stft(audio_array))),     \n",
    "        librosa.feature.spectral_centroid(y=audio_array, sr=sr).mean(),  \n",
    "        librosa.feature.spectral_bandwidth(y=audio_array, sr=sr).mean(),  \n",
    "        librosa.feature.spectral_rolloff(y=audio_array, sr=sr).mean(),    \n",
    "        librosa.feature.rms(y=audio_array).mean()       \n",
    "    ]\n",
    "    print(f\"Frequency features extracted: {len(features)}\")  \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def extract_impact_features(audio_array, sr=22050):\n",
    "    onset_env = librosa.onset.onset_strength(y=audio_array, sr=sr)\n",
    "    return [\n",
    "        np.max(onset_env), \n",
    "        np.mean(onset_env) \n",
    "    ]\n",
    "\n",
    "def create_training_data():\n",
    "    training_data = []\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            print(f\"⚠️ Missing directory: {path}\")\n",
    "            continue\n",
    "            \n",
    "        class_num = CATEGORIES.index(category)\n",
    "        audio_files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
    "        \n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"⚠️ No WAV files in {path}\")\n",
    "            continue\n",
    "            \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(path, audio_file)\n",
    "            try:\n",
    "                audio_array, sr = librosa.load(audio_path, sr=22050, duration=30)\n",
    "                features = extract_time_domain_features(audio_array) + extract_frequency_domain_features(audio_array, sr)\n",
    "                \n",
    "                \n",
    "                if len(features) != 13:  \n",
    "                    print(f\"⚠️ Invalid features for {audio_file}: {len(features)}\")\n",
    "                    continue\n",
    "                    \n",
    "                training_data.append([features, class_num])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {audio_file}: {str(e)}\")\n",
    "                \n",
    "    return training_data\n",
    "\n",
    "\n",
    "\n",
    "DATADIR = r'C:\\Users\\Abedi\\OneDrive - Student Ambassadors\\archive (7)\\combined_datasets'\n",
    "CATEGORIES = ['fall', 'not fall']\n",
    "\n",
    "training_data = create_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n",
      "Time features extracted: 8\n",
      "Frequency features extracted: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "DATADIR = r'C:\\Users\\Abedi\\OneDrive - Student Ambassadors\\archive (7)\\combined_datasets'\n",
    "CATEGORIES = ['fall', 'not fall']\n",
    "\n",
    "\n",
    "training_data = create_training_data()\n",
    "assert len(training_data) > 0, \"No data loaded! Check paths and files.\"\n",
    "\n",
    "X = [features for features, _ in training_data]\n",
    "y = [label for _, label in training_data]\n",
    "\n",
    "max_length = max(len(f) for f in X)\n",
    "X_padded = np.array([np.pad(f, (0, max_length - len(f))) for f in X])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequential_features(audio_array, sr=22050, n_mfcc=13):\n",
    "    mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Recall: 0.43\n",
      "Precision: 0.38\n",
      "F1 Score: 0.40\n",
      "\n",
      "XGBoost Performance:\n",
      "Recall: 0.57\n",
      "Precision: 0.50\n",
      "F1 Score: 0.53\n",
      "\n",
      "SVC Performance:\n",
      "Recall: 0.29\n",
      "Precision: 0.50\n",
      "F1 Score: 0.36\n",
      "\n",
      "GaussianNB Performance:\n",
      "Recall: 1.00\n",
      "Precision: 0.54\n",
      "F1 Score: 0.70\n",
      "\n",
      "Random Forest Performance:\n",
      "Recall: 0.86\n",
      "Precision: 0.86\n",
      "F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'SVC': SVC(probability=True),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "def evaluate_fall_detector(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return {\n",
    "        'recall': recall_score(y_test, y_pred, pos_label=1),\n",
    "        'precision': precision_score(y_test, y_pred, pos_label=1),\n",
    "        'f1': f1_score(y_test, y_pred, pos_label=1)\n",
    "    }\n",
    "\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_scaled, y_train) \n",
    "    metrics = evaluate_fall_detector(clf, X_test_scaled, y_test)\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall Recall: 0.43\n",
      "Fall Precision: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "\n",
    "clf = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)  \n",
    "\n",
    "print(f\"Fall Recall: {recall_score(y_test, y_pred, pos_label=1):.2f}\")\n",
    "print(f\"Fall Precision: {precision_score(y_test, y_pred, pos_label=1):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall Recall: 0.42857142857142855\n",
      "Fall Precision: 0.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "print(f\"Fall Recall: {recall_score(y_test, y_pred, pos_label=1)}\")\n",
    "print(f\"Fall Precision: {precision_score(y_test, y_pred, pos_label=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (124, 13)\n"
     ]
    }
   ],
   "source": [
    "assert 'X_train_scaled' in globals(), \"Run data preprocessing first!\"\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "\n",
    "feature_names = [\n",
    "    \n",
    "    'mean_amp', 'median_amp', 'variance', 'std_dev',\n",
    "    'skewness', 'kurtosis', 'zero_cross_rate', 'onset_count',\n",
    "    \n",
    "    \n",
    "    'stft_mean', 'spectral_centroid', 'spectral_bandwidth',\n",
    "    'spectral_rolloff', 'rms_energy'\n",
    "]\n",
    "\n",
    "assert len(feature_names) == X_train_scaled.shape[1], (\n",
    "    f\"Feature mismatch! Data: {X_train_scaled.shape[1]} vs Names: {len(feature_names)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\Scripts\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\Scripts\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\Scripts\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\Scripts\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\Scripts\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --matplotlib\n"
     ]
    }
   ],
   "source": [
    "pip install --matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xgb_model = XGBClassifier(scale_pos_weight=3.5, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "result = permutation_importance(\n",
    "    xgb_model, \n",
    "    X_test_scaled, \n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "pos = np.arange(len(sorted_idx)) + .5\n",
    "\n",
    "plt.barh(pos, result.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Feature Importance Ranking\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.1.3\n",
      "Uninstalling numpy-2.1.3:\n",
      "  Successfully uninstalled numpy-2.1.3\n",
      "Found existing installation: shap 0.46.0\n",
      "Uninstalling shap-0.46.0:\n",
      "  Successfully uninstalled shap-0.46.0\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap==0.44.0\n",
      "  Using cached shap-0.44.0-cp310-cp310-win_amd64.whl (447 kB)\n",
      "Requirement already satisfied: numba in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (0.61.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (3.1.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (4.67.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from shap==0.44.0) (1.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from tqdm>=4.27.0->shap==0.44.0) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from numba->shap==0.44.0) (0.44.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from pandas->shap==0.44.0) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from pandas->shap==0.44.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from pandas->shap==0.44.0) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn->shap==0.44.0) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn->shap==0.44.0) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap==0.44.0) (1.17.0)\n",
      "Installing collected packages: slicer, shap\n",
      "  Attempting uninstall: slicer\n",
      "    Found existing installation: slicer 0.0.8\n",
      "    Uninstalling slicer-0.0.8:\n",
      "      Successfully uninstalled slicer-0.0.8\n",
      "Successfully installed shap-0.44.0 slicer-0.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy shap -y\n",
    "!pip install numpy==1.26.4  \n",
    "!pip install shap==0.44.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "best_model = classifiers['XGBoost']\n",
    "\n",
    "joblib.dump(best_model, 'fall_detection_model.pkl')\n",
    "\n",
    "loaded_model = joblib.load('fall_detection_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution Analysis:\n",
      "Total samples: 124\n",
      "Fall samples (0): 97\n",
      "Non-fall samples (1): 27\n",
      "Class ratio: 0.78 : 0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "assert 'y_train' in locals(), \n",
    "y_train = np.array(y_train)  \n",
    "\n",
    "print(\"\\nClass Distribution Analysis:\")\n",
    "print(f\"Total samples: {len(y_train)}\")\n",
    "print(f\"Fall samples (0): {np.sum(y_train == 0)}\")\n",
    "print(f\"Non-fall samples (1): {np.sum(y_train == 1)}\")\n",
    "print(f\"Class ratio: {np.sum(y_train == 0)/len(y_train):.2f} : {np.sum(y_train == 1)/len(y_train):.2f}\")\n",
    "\n",
    "class_weights = {\n",
    "    0: len(y_train)/(2*np.sum(y_train == 0)),  # Fall class weight\n",
    "    1: len(y_train)/(2*np.sum(y_train == 1))   # Non-fall weight\n",
    "}\n",
    "\n",
    "best_model = classifiers['XGBoost'].fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    sample_weight=np.array([class_weights[label] for label in y_train])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X_arr = np.array(X_padded)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X_arr):\n",
    "    X_train_ts, X_test_ts = X_arr[train_index], X_arr[test_index]\n",
    "    y_train_ts, y_test_ts = np.array(y)[train_index], np.array(y)[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Total samples: 124\n",
      "Fall samples (Class 0): 97 (78.2%)\n",
      "Non-fall samples (Class 1): 27 (21.8%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution:\")\n",
    "print(f\"Total samples: {len(y_train)}\")\n",
    "print(f\"Fall samples (Class 0): {sum(y_train == 0)} ({np.mean(y_train == 0)*100:.1f}%)\")\n",
    "print(f\"Non-fall samples (Class 1): {sum(y_train == 1)} ({np.mean(y_train == 1)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "best_model =  XGBClassifier(\n",
    "    scale_pos_weight=3.5,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    eval_metric='logloss'\n",
    ").fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)  \n",
    "X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalXGBoost(XGBClassifier):\n",
    "    def _objective(self, preds, dtrain):\n",
    "        labels = dtrain.get_label()\n",
    "        grad = (preds - labels) * ((1 - preds) ** 2)  \n",
    "        hess = (preds * (1 - preds)) * ((1 - preds) ** 2)\n",
    "        return grad, hess\n",
    "\n",
    "focal_model = FocalXGBoost().fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced').fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "    X_fold, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_fold, y_val = y_train[train_idx], y_train[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.7619047619047619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fall       0.94      0.67      0.78        24\n",
      "    Non-fall       0.43      0.86      0.57         7\n",
      "\n",
      "    accuracy                           0.71        31\n",
      "   macro avg       0.68      0.76      0.68        31\n",
      "weighted avg       0.83      0.71      0.73        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred, target_names=['Fall', 'Non-fall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_weight = len(y_train)/(2 * np.sum(y_train == 0))  \n",
    "non_fall_weight = len(y_train)/(2 * np.sum(y_train == 1))  \n",
    "\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=non_fall_weight/fall_weight,  \n",
    "    eval_metric='logloss'\n",
    ").fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "No threshold meets precision target. Using default 0.5\n",
      "\n",
      "Optimized Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85        24\n",
      "           1       0.50      0.57      0.53         7\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.68      0.70      0.69        31\n",
      "weighted avg       0.79      0.77      0.78        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def optimize_threshold(model, X_test, y_test, target_precision=0.7):\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    \n",
    "    \n",
    "    valid_thresholds = thresholds[:len(precisions)-1]\n",
    "    \n",
    "    \n",
    "    meets_precision = np.where(precisions[:-1] >= target_precision)[0]\n",
    "    \n",
    "    if meets_precision.size > 0:\n",
    "        optimal_idx = meets_precision[0]\n",
    "        optimal_threshold = valid_thresholds[optimal_idx]\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.2f} (Index: {optimal_idx})\")\n",
    "        return (y_probs >= optimal_threshold).astype(int)\n",
    "    else:\n",
    "        print(\"No threshold meets precision target. Using default 0.5\")\n",
    "        return (y_probs >= 0.5).astype(int)\n",
    "\n",
    "model = XGBClassifier().fit(X_train_scaled, y_train)  \n",
    "y_pred_optimized = optimize_threshold(model, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nOptimized Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (124, 13)\n",
      "Enhanced shape: (124, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_impact_features(X, accel_indices=[0, 1, 2, 3]):\n",
    "\n",
    "    try:\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"Input X must be 2-dimensional\")\n",
    "            \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "  \n",
    "        accel_deriv = np.diff(X[:, accel_indices], axis=0)\n",
    "        accel_deriv = np.vstack([accel_deriv[0,:], accel_deriv])  # Pad first row\n",
    "        \n",
    "        \n",
    "        if 7 >= n_features:\n",
    "            raise IndexError(\"Column index 7 exceeds feature dimensions\")\n",
    "        resultant_force = np.sqrt(X[:, 3]**2 + X[:, 7]**2)[:, np.newaxis]\n",
    "        \n",
    "        energy = np.sum(X[:, accel_indices]**2, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        return np.hstack([X, accel_deriv, resultant_force, energy])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature engineering failed: {str(e)}\")\n",
    "        return X  \n",
    "\n",
    "X_enhanced = add_impact_features(X_train_scaled)\n",
    "print(\"Original shape:\", X_train_scaled.shape)\n",
    "print(\"Enhanced shape:\", X_enhanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (1.5.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Requirement already satisfied: xgboost in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "Successfully installed scikit-learn-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (1.6.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abedi\\human-analytics-research-codes\\tf_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade xgboost scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SklearnXGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.model = XGBClassifier(**kwargs)\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model.fit(X, y, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def __sklearn_tags__(self):\n",
    "        \n",
    "        return {\n",
    "            'binary_only': False,\n",
    "            'requires_fit': True\n",
    "        }\n",
    "\n",
    "model = SklearnXGBClassifier(\n",
    "    scale_pos_weight=3.5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model.fit(X_train_scaled, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xgb_model = XGBClassifier(scale_pos_weight=3.5, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "result = permutation_importance(\n",
    "    xgb_model, \n",
    "    X_test_scaled, \n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "pos = np.arange(len(sorted_idx)) + .5\n",
    "\n",
    "plt.barh(pos, result.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Feature Importance Ranking\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import xgboost as xgb\n",
    "\n",
    "class SklearnXGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.model = xgb.XGBClassifier(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model.fit(X, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_probs: (31,)\n",
      "Custom predictions shape: (31,)\n",
      "Unique values in custom predictions: [0 1]\n",
      "\n",
      "Classification Report for Custom Predictions:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84        24\n",
      "           1       0.40      0.29      0.33         7\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.60      0.58      0.59        31\n",
      "weighted avg       0.72      0.74      0.73        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = classifiers['XGBoost'].fit(X_train_scaled, y_train)\n",
    "\n",
    "y_probs = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Shape of y_probs:\", y_probs.shape)\n",
    "fall_threshold = 0.3  \n",
    "non_fall_threshold = 0.7\n",
    "if len(y_probs.shape) == 1:\n",
    "    y_pred_custom = np.where(\n",
    "        y_probs >= non_fall_threshold, 1,  \n",
    "        np.where(1 - y_probs >= fall_threshold, 0, -1) \n",
    "    )\n",
    "else:\n",
    "    \n",
    "    y_pred_custom = np.where(\n",
    "        y_probs[:, 1] >= non_fall_threshold, 1,  \n",
    "        np.where(y_probs[:, 0] >= fall_threshold, 0, -1)  \n",
    "    )\n",
    "\n",
    "print(\"Custom predictions shape:\", y_pred_custom.shape)\n",
    "print(\"Unique values in custom predictions:\", np.unique(y_pred_custom))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report for Custom Predictions:\")\n",
    "print(classification_report(y_test, y_pred_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability array shape: (31,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_cost_function(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    threshold = 0.5  \n",
    "\n",
    "    steepness = 100.0  \n",
    "    approx_pred = K.sigmoid(steepness * (y_pred - threshold))\n",
    "    \n",
    "\n",
    "    fp_approx = approx_pred * (1 - y_true)\n",
    "    fn_approx = (1 - approx_pred) * y_true\n",
    "\n",
    "    false_positives = K.sum(fp_approx) * 3.0\n",
    "    false_negatives = K.sum(fn_approx) * 1.0\n",
    "\n",
    "    loss = (false_positives + false_negatives) / K.cast(K.shape(y_true)[0], 'float32')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\Abedi\\Human-Analytics-Research-Codes\\tf_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/3 [==============================] - 3s 317ms/step - loss: 1.4863 - accuracy: 0.6264 - val_loss: 1.0083 - val_accuracy: 0.5217\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6837 - accuracy: 0.7473 - val_loss: 0.5284 - val_accuracy: 0.6957\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3535 - accuracy: 0.7912 - val_loss: 0.3092 - val_accuracy: 0.7391\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3442 - accuracy: 0.7912 - val_loss: 0.2704 - val_accuracy: 0.7391\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3235 - accuracy: 0.7912 - val_loss: 0.2636 - val_accuracy: 0.7391\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3573 - accuracy: 0.7912 - val_loss: 0.2620 - val_accuracy: 0.7391\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3189 - accuracy: 0.7912 - val_loss: 0.2614 - val_accuracy: 0.7391\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3186 - accuracy: 0.7912 - val_loss: 0.2612 - val_accuracy: 0.7391\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3578 - accuracy: 0.7912 - val_loss: 0.2611 - val_accuracy: 0.7391\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3256 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3185 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3202 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3255 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3477 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3221 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3348 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3222 - accuracy: 0.7912 - val_loss: 0.2610 - val_accuracy: 0.7391\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3218 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3298 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3287 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3253 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3221 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3182 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3252 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3185 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3220 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3184 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3274 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3459 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3298 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3222 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3202 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3279 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3366 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3206 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3263 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3219 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3256 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3223 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3204 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3264 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3390 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3410 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3201 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3184 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3359 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3221 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3221 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3296 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3348 - accuracy: 0.7912 - val_loss: 0.2609 - val_accuracy: 0.7391\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(None, X_train_scaled.shape[1]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=custom_cost_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "X_train_seq = X_train_seq.astype('float32')\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq,\n",
    "    y_train[10:],  \n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight={0: 1.0, 1: 3.5},\n",
    "    verbose=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
